How can we defind the multithreading behavior?

First, we need to clearify what threading means, well, threading is the idea of taking one single processes and run it multiple times at the same time, this is diffrent then multiprocessing, which refers to the ability of runing multiple diffrent processes at the same time.

Let take an analogie

imagine that we have a dog race, and we want them to go straight to the finish line. We can do this sequentially, so we will let one go, and when it passes the finish line, we let the other one go, and keep doing this until all the dogs have crossed the finish line. But if we decided to do this at the same time, and this what is refered as multithreading, we can line up all the dogs in gates, so that every dog is on its gate, then all we need, to do is to open up the gates doors at the same time, and all the dogs will run to the finish line at the same time also. So, with the second approch we will definetly gain more time.




Now, imaging that our dog race is like the itunes app, and our dogs are the downloading processes, how can we explain this in the processor and the memory view.

Well in the processor view, it does not change from the multiporcessing view, because the processor just know that it needs to get the stuff done seamingly at the same time. 

The diffrence between mutiprocessessing and multithreading in the processor view is that in multiprocessessing the processor will use diffrent cores to handel diffrent application, and for each application, it will use the allocated core threads for it sub-processes, so imagine that we have a quad core cpu, and we are lunching spotify, so the processor will handel the spotify application in the core, let say number 1, and now within the spotify we download a song and play another song at the same time, so now the processor will take the download sub-process and assign it to the thread number 1 of the core number 1, and song playback for the thread number 2 of the core number 1 for exemple. This is how in theory it is done.




Whereas in the memory view, now we don't have to devide the physical memory into sub-sets and attach each downloading process it own vertual memory because all these sub-processes are under one application run time, so they all share the same set of instruction, and eventuelly the same global variables inherited from the application stack frame. So all these littel processes share some information, and if we go and give them seperated chunk of the main memory we will find our selves duplicating bytes and bytes of data everytime another sub-process is called withing the same global function, and that will cause memory leaks and also slowdowns, and we don't want that.



So what happens, is that the operating system, will allocate some vertual memory of the each application call, and then will figure out how to manage this chunk of memory to hundel all the inherited processes form this application call. 

So basicly, it will draw the code and data segment, which will be shared ressources between all the sub processes, because in one application the assembly code and the global set of data remains the same. The trickiest part, it to figure out how to isolate each sub-processes behaviour from one to another in the stack and the heap segments.

Hopefully, the operating system is smart enough, and what is will do, is to attribute each of the sub-processes a thread id, this peace of information tells the processor, what is the allowed and the restricted chunks of the stack and the heap segements whithin the vertual memory allocated the global application that is runing.



Understanding the multithreading in depth:

So as we said, for every application call, we likely end up with some sub-processes flying around in the cpu cores threads.


We need to know that there is a time slide within all the threads are active, let say that this time slide is equal to 100ms, so every thread has 100ms to make some progress in the assembly code execution before it is blocked out by the processor which will switch out to another thread.

We also need to know that, every thread has a state, or some sort of memory ability that give it the chance to continue right where it was before getting pulled off by the cpu.


Adding to that, we have to keep in mind, that cunccurent programming is used for treating the same problem via seperated agents (threads) at the same time, so it not unlikely that we are using multithreading to interact with one set of data, lets call this data the master ressource.


To go more in depth let introduce this analogy: 

Imagine, that we have an airline company, and for a specific flight we have 150 seats availible, and we want to sell them as quickely as possible.

The best approach, is to use concurency, and for that we will take 10 agents, and tell them that we have these 150 seats to sell.

So all the agents sells the tickets to get the job done.

With this approach, we would fix that problem occured when an agent is holded by a very long disscution with a client, because all what happens is that the other agents will take care of the rest. Unlike the approach of giving each agent 15 tickets to sell independently from the others. 

So now that we found the best approach in the real world, let try to implement it in the cuncurrent programming scope.


So our tickets will be replaced by an int called availibe tickets and which will be equal in the beginning to 150

Our 10 threads will be our 10 agents

So now every thread is working to sell the tickets, but the diffrence in real world and  in the programming world, is that the agent is not pulled off of it office like the thread will be, and agents can communicate between each other and can see what the others are doing.

The reason why I said that, is that if we take the case where our master ressource, tickets availible, is now equal to one. And all the threads will come up to take care of it, at the end of this last cycle our master ressource will not be equal to 0 as it should be, but it will be equal to -9, because all the threads will decrement this integer by one.

So the obvious solution is to add some sort of conditional in our programe to prevent that, but remeber that each thread is reading through the assembly code, so even if we add up this conditionnal it does remain a problem, because we are not protected from the case where the master ressource is like 1, an thread is pulled off by the cpu just after reading the conditional assemby code, and it will switch our back to it later, and when the processor comes back to it, this thread continues directly from where it was, so eventually it will decrement the master ressource by one, and that even if another thread did that while it was pulled off.

And imagine this case will repeat n times, in which n is equal to our number of threads, so in this analogy the worst case senario is to end up with the master ressource equal to -9 .


So to prevent that, we need to intorduce a concepte, that basicly will lock the reserved assembly code for decrementing the shared ressource from other threads if one thread is already reading through it.

This concepte is called a binary lock, which locks a specific chunck of assembly code to be read just by a single thread at each time slide.

To understand it more, let take this analogy:

imagin that we have 3 persons, and we have one bathroom, obviously we want just one person in the bathroom at a given time.
So likely we can use a door lock, if a person goes in, she will lock the door behind it, and will unlock it to allow the other ones to go in. 


So with is, we can translate our persons into threads and our bathroom as the specific critical assembly code area, and the door lock as our binary lock concepte.







link : https://www.youtube.com/watch?v=OGHN_zVTMMo&list=PL9D558D49CA734A02&index=16

























